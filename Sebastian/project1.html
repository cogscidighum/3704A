<!DOCTYPE html>
<html>
  <head>
    <title> Sebastian Sharma Project One </title>
  </head>
  <style>
    body {
      font-family: 'Verdana';
      background: #b30059;
      color: #f3fff0;
      text-align: center;
      margin-top: 10%;
      margin-bottom: 10%;
      margin-left: 25%;
      margin-right: 25%;
    }


  
  </style>
  <body>
    <h1> Project One: Tom Scott & Truth </h1>
    <p>
      <iframe src="//www.youtube.com/embed/leX541Dr2rU?t=1s&amp;ab_channel=TheRoyalInstitution" width="560" height="314" allowfullscreen="allowfullscreen"></iframe>
    </p>
    <br></br>
    <h2>Intro</h2>
    <p> In his talk There is no Algorithm for Truth, Tom Scott discusses the use of algorithms in filtering YouTube content; to determine suitability for advertisers, keep users on the platform, and encourage more users. He pointed out biases coming out of the algorithms, appearing to target minority groups, such as queer communities as they proportionally spoke about sensitive issues more. Additionally, the methods which YouTube uses to keep users on the platform as long as possible to put out more advertising, favouring clickbait, and algorithms ultimately highlighting relatable, authoritative voices over knowledgeable ones. </p>
    <p> Tom calls into question the ways we moderate online communities, contrasting methods of total freedom with methods of total restriction. With the impossibility of making an algorithm to recognise truth, he sees the solution as algorithms focusing on helping humanity survive in the long term, while still allowing for entertainment. </p>
    <p> As a YouTuber, Tom Scott is directly impacted by the algorithms he speaks of, and recognises his privilege as a British white man which aided him to his (at the time) status of 2 million subscribers; his critique of the system he depends on, and consideration of the decisions that led to the current situation makes him appear as a respectable voice, making the most of his platform. </p>
    <p> Together the evidence Scott uses - the ways that training content algorithms lead to bias, that advertisers favour clicks, and that voice is valued over knowledge - support his well, however as a user of such platforms opposed to being a creator, I see things in a different light. </p>
    <h2>Analysis</h2>
    <p> The start of the talk looks at the introduction of content-filter algorithms on YouTube. Tom Scott introduces the idea of advertiser-friendly content, free of sensitive topics such as porn or violence; the original algorithms were trained on 100% advertiser friendly, and 100% unfriendly content to learn patterns in data. Immediately, they presented a bias. Scott presents the example of channels discussing queer issues being more likely to be flagged as they talk more about sex (11:07). The algorithms recognising an association between inappropriate topics and LGBT communities led to them being unfairly restricted. Tom gauged this calling into question other systemic issues that the algorithm may have learned, such as if it’s responsible for (in 2018) all top 10 YouTube creators being white men in their 30’s.This statistic is no longer true (Social Blade, 2023). </p>
    <p> Scott’s analysis of content filters raises the question of what should be allowed. He understands that there is no set answer to this question, and that’s the premise of his talk. Algorithms can’t say if something is good or bad, rather they are built off of social beliefs, and being the creation of corporations, they reflect beliefs of those companies. Whether or not they truly reflect beliefs is what Scott is questioning, seeing how they’re able to pick up their own biases as they form their knowledge; these are biases consequence to their instruction. Seeing how easily the algorithm picked up a prejudice against LGBTQ communities, the topic of what made the most popular YouTubers the most popular YouTubers can be called similarly be asked, as perhaps the abundance of decent content from white men in their 30’s made the algorithm trust them more. </p>
    <p> Next, recognising YouTube as a marketing platform, Scott discusses the ways Machine Learning is used to recommend videos. YouTube claimed to “reward high-quality videos that keep people on platform” (13:13). This is to go against videos padding for length with valuable content just at the end. Scott’s research led him to understand the design of the recommendation algorithm is meant to keep the user on the platform as long as possible. Videos that draw people into the platform are rewarded, and videos that people watch more of are awarded. </p>
    <p> As a creator, Scott compares the algorithm to a skinner box, where he makes changes, and sees the effects, and tries to optimise the reward (15:39). My opinion as a participant in the platform has me somewhat self-conscious about what I’m viewing. I keep in mind a similar idea of a skinner box, but playing the role of Skinner. If I view something I’m aware I will be suggested similar content, so I only view things under my account that I want similar things to. The fact I do this relates to Scott’s claim that the platform needs both clickbaity, and detailed content to survive. I tend to let the detailed content fuel my recommendations, while less meaningful things I don’t let get added to my history. This method has me seeing myself try to claim some sense of control over the algorithm Scott speaks of, rewarding my opinion of quality content, by allowing it to pass onto YouTube's understanding of quality content. </p>
    <p> My method of viewing YouTube appears somewhat distrusting of the algorithm if I look at it through Tom Scott’s lens. While him keeping in mind the algorithm guides his business, my keeping in mind the algorithm gives me a sense of control. With the platform’s intent of marketing, trying to get people to constantly view content, my hesitance of what I watch is exactly what the algorithm is trying to fight - perhaps it’s because I am not allowing it to have every part of me, a boundary in a sense. Tom’s talk is pretty neutral towards the algorithms, recognising they’re necessary to make the platform succeed, and in the same way while I agree, I make these boundaries with them. I am content to have them present, but want them to act in some specific way, which they won’t do as they’re tuned for business, so I must operate some control with them. </p>
    <p> With an understanding of the underlying systems, Tom began to question the society that interacts with YouTube. He sees the platform holding content able to argue for any viewpoint a user might want to hear. The underlying theme of the talk is content can’t be judged on truth, so he recognises engagement as the metric. Between television, personal experiences, and YouTube, Tom claims loyal audiences don’t care about a speaker's professionality, rather they care about them, and how engaging they are as a person (27:20). He doesn’t use any words to do with likeability, more considering it as an affection to one-hit-wonders, and the development of a parasocial relationship. Seeing how people flock to livestreams just to get noticed, or Patreon systems being about friendship rather than just supporting content. </p>
    <p> My thoughts are agreeing with Tom. Through his talk, he made frequent privilege checks, recognising his status as a white man in his 30’s - the most successful group on YouTube. While he put this up to the content filtering algorithm, he was not wrong in saying I clicked on the video because of him. I’m interested in the idea of algorithm’s trying to find some sense of what’s right, but for such a long video on the subject, what ultimately pushed me to watch it was that he was the one talking. I’ve seen previous content of his and enjoyed it, so seeing him talk about something I’m interested in (and relevant to the class), I was happy to watch. I was not entirely aware that he had his degree in linguistics, I’ve mostly watched his out and about things with a bit of his math content. Knowing now that those are all researched videos rather than direct knowledge does not really impact my respect for him. He sources things, and I think that’s where my respect lies; research is a common practice, and as long as people are gaining information from reliable sources, there is no reason to denounce people talking about other people’s knowledge. The components of personal thoughts, such as those Tom shared in his talk, or personal takes I’ve seen in video essays aren’t ultimately the reason I watch things. I tend to lean towards creators who talk about things I’m interested in, or present things in a weird and creative way. I don’t tend to watch people based on their looks, personal ideas, or voice, but perhaps that’s YouTube only giving me things that fit my taste. Through these personal experiences, I can agree that knowledgeably isn’t relevant in the sharing of content, however that doesn’t mean I have no values at all as a viewer. Additionally, to Tom’s suggestion that forming parasocial relationships is a goal in content creation, I can only speak as a viewer. I do not seek them out, but when I watch someone frequently, and choose to subscribe to them, I let myself form a mental character of them; based on what they talk about, their opinions, and beliefs. I feel Tom’s ideas suggest creators try to be as agreeable as possible with viewers, however agreement isn’t a personal value of mine, rather I feel that’s him reflecting his own beliefs in his talk. Instead, I feel like honesty is my key. </p>
    <p> The last idea Tom discusses is how moderation should happen in a world where money comes from likeable people saying what others want to hear. He compares two extremes: a Nazi Bar, and an Echo Chamber. Seeing them as different ends of a gradient; Nazi bars have no moderation, allowing for people to say anything they want, regardless of respect or intensity; Echo Chambers are ultra moderated, leading to a point where only one idea may be shared, polarizing members. Different platforms try to place themselves somewhere along them as a gradient, where they might hurt the least people and make the most money. </p>
    <h2> Conclusion</h2>
    <p> Tom’s ultimate conclusion to the design of algorithms moderating platforms is that they should try to protect humanity, something he sees as a distant dream. I agree that profit motives aren’t the most caring, and promoting humanity is much more tolerable focus, and more realistic than seeking out truth. While he argued for this because of the easiness for bias and other questionable consequences when designing algorithms, I feel there’s another side of it, as a focus on promoting humanity can improve quality. In moderation-for-profit groups have to decide how much discourse do they want to allow, placing them on the Echo-Chamber Nazi Bar scale, making the choice that would attract the most users. A focus on trying to benefit humanity instead doesn’t necessarily decide to let everything or nothing be said, perhaps it just tries to reduce hurt, or increase conversation so the most ideas are possible, with the least amount of harm. In a way this is what creating rules is for, to keep people safe; safe platforms are what companies want to attract users, but if companies instead sought out safe platforms to help people, then they may work in a more quality over quantity way, still allowing participants to be willing to engage. </p>
    <br></br>
    <h3> References:</h3>
    <p>Social Blade. 2023. YouTube Stats | YouTube Statistics. Social Blade. https://socialblade.com/youtube/</p>
  </body>
</html>
