<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>My Project One HTML Page: Bret Weinstein - Celebrity Scientist </title>
    <style>
        body {
            background-color: #C5EF91;
            font-family: 'Helvetica', sans-serif;
            font-size: 16px;
            color: black;
        }

        h1 {
            font-size: 45px;
            color: #674521;
            text-align: center;
        }

        h2 {
            font-size: 24px; 
            color: #674521;
            text-align: left;
        }

        p {
            font-size: 18px;
            text-align: left;
             margin-bottom: 20px;
        }

        hr {
            border: 1px solid #674521;
        }

        .right-aligned {
            float: right;
            margin: 0 0 10px 10px;
        }
    </style>
</head>
<body>

    
    <h1 style="text-align: center;">The Dawn of Artificial General Intelligence</h1>
    
       <hr>
    <h2>Abstract</h2>
   
   <p>
        Bret Weinstein is a polarizing character in the world of online celebrity scientists. He has studied and worked in the field of evolutionary biology for decades and currently aims to apply the tools of science he’s familiar with to explore questions about human nature and our future.
    </p>


    <img src=https://images.squarespace-cdn.com/content/v1/62cc5ba9ac4a585b30992081/ffd28fd0-7bc0-49c3-95c3-f9bd980a1214/bre-bio.png?format=750w alt="Dr. Bret Weinstein" width="50%" height="50%" class="right-aligned">
<p>
Along with his friend Alexandros Marinos, Weinstein discussed this on a recent podcast entitled “The Dawn of AGI” in an episode where they talked about the possibilities and risks posed by the use of artificial intelligence, as well as the challenges in handling such innovations in a responsible manner. In this friendly debate, Weinstein takes a more wary perspective whereas Marinos tends to be more optimistic about the possibilities of future uses of AI. Although the title of the podcast is the ‘Dawn of AGI’ (Artificial General Intelligence), it was acknowledged that this does not (yet) exist, and Marinos has doubts that this sort of generalized machine intelligence is possible at all. 
</p>
    
     
         <hr>
             <h2>Video Summary</h2>
<p>
Weinstein begins by explaining his concerns about Marinos’ use of the term ‘Doomer’ regarding the conversations surrounding the future of AI: naming and labeling individuals as 'doomers' lumps together those who believe that our next steps forward must be carefully considered with those individuals who believe that use of AI is an existential threat and must be stopped. A significant portion of the population share genuine concern about the implications of AI and the advent of AGI, yet these valid concerns can be minimalized with oversimplistic labels. 
Marinos clarified that no term can ever be considered perfect; language involves compression of ideas and nuance loss. Using a reductionist label to group individuals together when they hold diverse perspectives is unfair, yet, words remain our primary means of communication so finding the correct words for our situation becomes essential. He suggests the use of the term “AI Safetyists” as a more inclusive phrase to be considered.  
</p>
<img src=https://as2.ftcdn.net/v2/jpg/04/83/92/61/1000_F_483926108_aSPFvor6HpWhnJhWJnIFHrpf5fwS6SBC.jpg?format=500w?format=750w alt="Stock photo: Asteroid" width="25%" height="25%" class="right-aligned">
<p>
AI Safetyists have valid concerns, and with these apprehensions come inevitable suggestions for how to proceed with caution. Marinos explains that a common suggestion is to establish a regulatory body akin to the World Health Organization or the World Trade Organization, perhaps a WAIO – World Artificial Intelligence Organization. However, when it comes to risk assessment, we encounter the challenge of single points of failure or control: a critical element that, if compromised, can cause an entire system to stop functioning. Marinos uses the example of the planet as a single point of failure – if something catastrophic happened to Earth, such as a big enough asteroid hitting the planet, it would lead to our demise. We rely on that single point and we have no redundancy, backup systems, or contingency plan. In risk management, it is important to mitigate the single points which could lead to the downfall of the system. He insists that a WIAO would become a single point of failure and an obvious target if a sentient AGI were to come into existence. He stresses that it is never a good idea to rely on a single point of failure as a solution to any complex problem. 
</p> 

<img src=https://as2.ftcdn.net/v2/jpg/06/06/49/59/1000_F_606495985_o9E7BYpfwp58l3IDvnv9lfn8iBHHq2dF.jpg?
format=500w?format=750w alt="Stock photo: Treaty " width="25%" height="25%" class="right-aligned">
<p>
Another suggestion is to mirror humanity’s attempt to contain nuclear weapons, such as the nuclear non-proliferation treaty. This suggestion, however, ignores the fact that this treaty has not produced effective results. There are presently 50 more nations possessing nuclear capabilities than in 1970 when it was first written. Secondly, its existence didn't deter the countries involved from advancing their nuclear arsenals. It is far from a perfect example for a solution to this new problem to be based upon. Marinos suggests that it is simply against human nature to destroy access to powerful things, even if those powers are ‘not intended’ for any specific use.
</p>
<p>
Weinstein agrees with this point, and stresses that a non-authoritarian approach to the situation must be considered, albeit without offering concrete suggestions on how to achieve this. His focus is on avoiding what he calls Draconian measures. He states that transferring power to those who believe they have exclusive knowledge because they have thoroughly assessed the risks is a dangerous step to take. Whether its naiveite or something more sinister, some individuals have used debateable tactics to achieve their desired results in their calculations. He draws parallels between the recent pandemic handling and severe suggestions for attempting to control the use and development of AI. He reminds his listeners that it's wise to be wary of relinquishing control to someone who displays unwavering certainty in situations where such certainty is hardly justified. Weinstein warns that if someone holds a belief that a disastrous outcome is 99% certain, a shift in perspective occurs and actions that were unthinkable at a 90% certainty of doom become highly rational. 
</p>
<img src=https://as2.ftcdn.net/v2/jpg/00/59/78/71/1000_F_59787119_tHZScg0doTJJpv9DfaLpDS4dzzrJ6MGq.jpg?
format=500w?format=750w alt="Stock photo: Robot" width="25%" height="25%" class="right-aligned">
<p>
Marinos reminds Weinstein and his viewers that programmers have attempted to enhance these systems into more capable autonomous agents and the initial amazement inevitably gives way to limitations in functionality as we stack uncertain elements upon each other. The accumulating uncertainties eventually overwhelm the system, leading to failure. It's not a catastrophic world-dominating type of failure, but rather a software glitch where the system fails to perform as expected and runs into errors. This type of reasoning, however, completely ignores the fundamental nature of progress and innovation, so its reckless to believe that because something hasn’t happened before, it never will. Marinos entertains the idea of a sentient AGI for the sake of thought experiments such as ‘Roko’s Basilisk scenario’. 
</p>
<p>
The scenario originated in game theory and describes the emergence of cooperation without direct communication between parties over a large gap. So, without any messages being sent, cooperation over vast spans of time or space becomes possible by predicting what the other party would in theory wish for you to be doing. When at last the parties meet, the previous actions dictate whether punishment or reward is warranted. From this perspective, one can imagine that we are currently responsible for how a sentient AGI would view our current actions regarding bringing it into existence at some point in the future. Present actions may influence how the AGI perceives individuals when it materializes, motivating early efforts to ensure a positive reception. Marinos doesn’t subscribe to this concept but uses it to illustrate the complexity of thought surrounding the issue at hand. 
</p>
<img src=https://as1.ftcdn.net/v2/jpg/03/66/19/40/1000_F_366194093_bCnShjRfxoMpJqGU4ES41iQpgxRZQtKs.jpg?
format=500w?format=750w alt="Stock photo: Paperclips" width="25%" height="25%" class="right-aligned">
<p>
The outcome of this thought experiment aligns with Weinstein’s proposed five outcomes of AI, the last three of which he states are guaranteed. The first involves malevolent AGI such as the one imagined in the Basilisk scenario, where AGI systems might perceive humans as adversaries or competitors and seek to eliminate or enslave them. Although not guaranteed, he states that this possibility is worth considering. The second scenario is misaligned AI, in which imperfect instructions lead to disaster. This concept is explored in the classic “Paperclip Maximizer” example: another thought experiment that explores a possibility that if an AI is programmed to make as many paperclips as possible, it may eventually destroy the world gathering materials to make more paperclips. Although this example is deliberately extreme, it entails AI interpreting objectives in destructive ways and, while not certain, this presents a potential existential threat that demands careful thought. These two scenarios are not guaranteed, Weinstein states, but such possibilities need to be acknowledged when we humans are toying with such massively capable technologies. 
</p>
<p>
Weinstein’s three assured catastrophes generating from the use of AI include malicious use against innocent individuals in forms such as biased algorithms and surveillance, which is occurring already. The second guaranteed catastrophe Weinstein insists will occur is individual as well as collective mental health issues arising from the confusion about what is real if we are constantly interacting with AI systems that mimic human interaction and emotions. Lastly, the most straightforward catastrophe he is sure will result from this technology is the destabilization of society due to AI replacing human jobs, leaving civilization with no clear plan for maintaining functionality when a significant portion of the population lacks productive roles. 
</p>
<hr>
<h2> Reflection </h2>
<p>
In watching this talk, it feels necessary to point out that realistically, it is impossible to assign percentage likelihoods of outcomes to questions regarding the peril of humanity at the hands of AI, so it is frustrating to hear people discuss with such certainty that there is such-and-such percent chance when there are thousands of factors at play which cannot be accounted for in these calculations.
</p>
<p>
Weinstein’s five outcomes can seem extreme; however, his proposed three guaranteed outcomes of AI are reasonable. I would add to his list perhaps a combination of his fourth and fifth points: If AI exists which can do everything a human can do but better, what are we then able to strive for as humans? Not for employment, but for fulfillment. It may become demoralizing to put in efforts to learn skills such as writing or visual arts if works better than those which can be created by our species can be materialized with the click of a button. Even today, artwork being created by computers is astounding, and language models such as ChatGPT are being used more and more frequently to produce content. These systems will likely become even more impressive with further use, as they are continuously learning from their inputs and outputs. 
</p>
<p>
The conversation between Weinstein and Marinos highlights the importance of nuanced discussions about new technologies such as artificial intelligence. They advocate for precise terminology, caution against centralization, and emphasize the unpredictability of AI development. Dr. Weinstein’s predictions about the consequences of AI use may or may not come into fruition, but this discussion encourages an approach AI's future with responsibility and consideration. Unfortunately, its far easier to list possible detrimental outcomes of this technology than it is to suggest realistic and effective recommendations for how to responsibly proceed in a way that minimized harm. However, the only way to uncover solutions to these problems is to continue to engage in multifaceted conversations which bring together a variety of perspectives, and so Dr. Weinstein is playing an important role in embracing this powerful technology with caution. 
</p>
 <hr>
<h2>Watch on YouTube:</h2>

    <iframe width="550" height="350" src="https://www.youtube.com/embed/YeucEiOKdiM" frameborder="0" allowfullscreen></iframe>

 <hr>
<h2>Action VIIa:</h2>

<iframe width="550" height="350" src="https://cogscidighum.github.io/3704A/Oak/ACogsphere.html" frameborder="0" >
</iframe>

 <hr>
<h2>Action VIIb:</h2>

<iframe width="550" height="100" src="https://cogscidighum.github.io/3704A/Oak/BCogsphere.html" frameborder="0" >
</iframe>

<hr>
<h2>References:</h2>

<p>
Stock photos, royalty-free images, graphics, vectors &amp; videos. Adobe Stock. (n.d.). https://stock.adobe.com/ 
</p>
<p>
Weinstein, B. The darkhorse podcast with Bret Weinstein + Heather Heying. Dr. Bret Weinstein. (n.d.). https://www.bretweinstein.net/darkhorse-podcast 
</p>
<p>
Weinstein, B. (n.d.). https://www.bretweinstein.net/  
</p>
<p>
Weinstein, B YouTube. (2023). The Dawn of AGI: Bret Speaks with Alexandros Marinos on the Darkhorse Podcast. YouTube. Retrieved October 25, 2023, from https://www.youtube.com/watch?v=YeucEiOKdiM.

</p>

</body>
</html>
