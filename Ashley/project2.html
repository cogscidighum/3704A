<!DOCTYPE html>
<html lang="en">
<html>
<head>
<style>
body {
  background-color: lightgrey;
}
h1 {
  text-shadow: 2px 2px 5px red;
}
</style>
</head>
</body>
</html>

<body>

<meta charset="utf-8" />
  <h1 style="color:pink;"> CGSC 3704 Project 2 </h1>
  <h2 style="font-family:Arial;"> Ashley Navas - 101226785 </h2>

<hr> 
<h3> Technology knows what you're feeling </h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/HW2SSoYteIs?si=XifrDzzbfWp5dT81" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


<hr>
<h3> Poppy Crum and the Future of Sound </h3>
<iframe width="560" height="315" src="https://www.youtube.com/embed/d4zyk7z5oh0?si=Fo4jcb6MWjKFy-We" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Alistair Croll is interviewing Poppy Crum regarding her predictions of sound, specifically in music. She has computer research experience in music & acoustics at Standford University and took her undergrad in violin performance. We have more data access to music than ever and with general science & sensory perception, we can change the music industry. She is the chief scientist at Dolby, so she mainly focuses on how they build algorithms. Dolby Atmos is audio that is transformed for the cinemas, the audio in the film is mixed and presented in a way that is modified based on the content in the film. The sounds can be mixed with data that carries a specific location to where the vibrations will go. 
This could also be used with music, as the metadata within the music can give information about where the sounds, percussion, bass, etc. can be located and can give a personalized view of the music to the artist’s intent. They have tried this with DJs with rendering algorithms that carry the data stream based on how loud the sound is, where the sound goes, and how diffused the noise is. The way we listen to music today is very different from a few decades ago when we are now able to access unlimited music at all times. We can also personalize our auditory experiences when we listen to music due to metadata encoding & access to complex neural models that are specific to the individual. However, there is an issue with computer-generated sounds inciting emotions and could lead to disconnect from the music, as it isn't human-made. Crum is particularly interested in the neuroscience of making sound and behaviour, and how music becomes a soundtrack in your life that builds your context that harnesses the contextual process that you can control. For example, listening to music while you’re working can affect your attention process and information processing. Regarding the future of music, she predicts that music will be more interactive using virtual and artificial reality. Using VR, we could have interactive music concerts and use Dolby Atmos to have the music change based on where the individual is located in the virtual reality.</p>

<hr>
