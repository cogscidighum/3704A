<!DOCTYPE html>
<html lang="en">
<html>
<head>
<style>
body {
  background-color: lightgrey;
}
h1 {
  text-shadow: 2px 2px 5px red;
}
</style>
</head>
</body>
</html>

<body>

<meta charset="utf-8" />
  <h1 style="color:blue;"> CGSC 3704 Project 2 </h1>
  <h2 style="font-family:Arial;"> Ashley Navas - 101226785 </h2>

<hr> 
<h3> Technology knows what you're feeling </h3>
<iframe width="460" height="215" src="https://www.youtube.com/embed/HW2SSoYteIs?si=XifrDzzbfWp5dT81" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


<hr>
<h3> Poppy Crum and the Future of Sound </h3>
<iframe width="460" height="215" src="https://www.youtube.com/embed/d4zyk7z5oh0?si=Fo4jcb6MWjKFy-We" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Alistair Croll is interviewing Poppy Crum regarding her predictions of sound, specifically in music. She has computer research experience in music & acoustics at Standford University and took her undergrad in violin performance. We have more data access to music than ever and with general science & sensory perception, we can change the music industry. She is the chief scientist at Dolby, so she mainly focuses on how they build algorithms. Dolby Atmos is audio that is transformed for the cinemas, the audio in the film is mixed and presented in a way that is modified based on the content in the film. The sounds can be mixed with data that carries a specific location to where the vibrations will go. 
This could also be used with music, as the metadata within the music can give information about where the sounds, percussion, bass, etc. can be located and can give a personalized view of the music to the artist’s intent. They have tried this with DJs with rendering algorithms that carry the data stream based on how loud the sound is, where the sound goes, and how diffused the noise is. The way we listen to music today is very different from a few decades ago when we are now able to access unlimited music at all times. We can also personalize our auditory experiences when we listen to music due to metadata encoding & access to complex neural models that are specific to the individual. However, there is an issue with computer-generated sounds inciting emotions and could lead to disconnect from the music, as it isn't human-made. Crum is particularly interested in the neuroscience of making sound and behaviour, and how music becomes a soundtrack in your life that builds your context that harnesses the contextual process that you can control. For example, listening to music while you’re working can affect your attention process and information processing. Regarding the future of music, she predicts that music will be more interactive using virtual and artificial reality. Using VR, we could have interactive music concerts and use Dolby Atmos to have the music change based on where the individual is located in the virtual reality.</p>

<hr>
<h3>Poppy Crum at AudaCities Humanity Design Summit </h3>
<iframe width="460" height="215" src="https://www.youtube.com/embed/0YvFCwb6Nyw?si=8Pl22SKzThGbR5HJ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<p>Crum works on the intersection of AI, empathetic technology and sensory data science and she is speaking at the Humanity Design Summit about using technological advances to make a city thrive. She is passionate about personalization in technology, as many technologies, such as our phones, are built in a one-size-fits-all box and assumes that we never move and does not consider what is happening the the space mentioned. She considers using sensors as a method to be interact and intelligent with our environment with the help of machine learning and AI, we could be able to get unique sensory experiences from the user and use these technologies to adjust them to the user. Technology is about using tools that optimize our internal states to better achieve our goals, however currently lacks personalization for uses and is made in a one-size-fits-all model. There are unique patterns of application with sounds by using morphology, the shape of our body and internal states. Machine learning & AI are central to her work for a need for customization. The general public having trust & transparency in algorithms is important, especially regarding obtaining data that is subjective to the user with their personalized data. She argues that we should accept the freedoms that come with obtaining personal data, as a way to elevate everyone's experiences. There is a bias in technology in how we build training sets, and it is important to be transparent about that bias since the majority don’t think about it. We can use technology in homes as a way to make individuals stay in their homes for as long they can as they age by focusing on physical/mental needs in the home with technologies. By improving our homes, we would be able to then be able to adjust it for communities and then cities by having intelligent spaces that are connected with our environments Having an environment that optimizes our needs, such as in the workplace. By dynamically changing the sounds/space of the room based on the environment, it can optimize our productivity. In 2019, Crum was consulted for the Sound of Silence film, which is about a man who listens in rooms and tries to solve their problems based on the vibrations within the rooms, due to her accurate representations of music and rich experiences with auditory senses. Although it is fictional, there is some reality in it, as our environment impacts our brain, creating contextual clues that adjust with us.</p>

<hr>
