<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Project 2</title>
<link rel="icon" href="https://www.iconninja.com/files/205/914/484/leather-brown-folder-icon.png">

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body {
  background: #D4C9BC;
  font-family: "Trebuchet MS";
  font-size: 17px;
  color: #362B21;
  text-align: left;;
}


h1, h2, h3, h4, h5 {
        font-family: Georgia, serif;
        text-align: center;
      }
.sidenav {
  height: 100%;
  width: 260px;
  position: fixed;
  z-index: 1;
  top: 0;
  left: -200px;
  background-color: #362B21;
  overflow-x: hidden;
  padding-left: 20px;
  opacity: 0.8;
}


.sidenav a, .dropdown-btn {
  padding: 6px 8px 6px 16px;
  text-decoration: none;
  font-size: 20px;
  color: white;
  display: block;
  border: none;
  background: none;
  width: 100%;
  text-align: left;
  cursor: pointer;
  outline: none;
}

.sidenav:hover {
		left: 0;
        opacity: 1.0;
}

.sidenav a:hover, .dropdown-btn:hover {
  color: #f1f1f1;
}


.main {
  margin-left: 200px; 
  font-size: 20px; 
  padding: 0px 10px;
}


.active {

  color: white;
}

.dropdown-container {
  display: none;
  padding-left: 8px;
}


.fa-caret-down {
  float: right;
  padding-right: 8px;
}

.accordion {
  background-color: #886248;
  font-family: "Trebuchet MS";
  color: white;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
  transition: 0.4s;
}

.active, .accordion:hover {
  background-color: #ad8468;
}

.accordion:after {
  font-size: 30px;
  content: '\002B';
  color: #777;
  font-weight: bold;
  float: right;
  margin-left: 5px;
}

.active:after {
  content: "\2212";
}

.panel {
  padding: 0 18px;
  background-color: #D4C9BC;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.1s ease-out;
} 


@media screen and (max-height: 450px) {
  .sidenav {padding-top: 15px;}
  .sidenav a {font-size: 18px;}
}
</style>
</head>
<body>

<div class="sidenav">
      <br>
      <a href="#Abstract">Abstract</a>

      <button class="dropdown-btn" href="#Videos">Videos 
        <i class="fa fa-caret-down"></i>
      </button>
      <div class="dropdown-container">
        <a href="#Utopia">Eugenics & the Promise of Utopia through AGI</a>
        <a href="#Dark">The Dark Side of AI</a>
        <a href="#Google">Timnit Gebru's Firing & Google</a>
      <a href="#Limitations">Limitations of AI</a>
        <a href="#EthicalAI">Ethical Artificial Intelligence</a>
      </div>

      <button class="dropdown-btn">Research<i class="fa fa-caret-down"></i>
  </button>
   <div class="dropdown-container">
    <a href="#Responsible">Responsible AI</a>
    <a href="#Artists">AI Art and its Impact on Artists</a>
  </div>
      <a href="#SocMed">Social Media</a>
      <a href="#Analysis">Analysis</a>
      <a href="#Reflection">Reflection</a>
      <a href="#References">References</a>
</div>

<div style="margin-left: 80px; margin-right: 80px;">
  <br>
    <h1 id="Abstract">Timnit Gebru : Computer Scientist Researching Algorithmic Bias</h1>
    <p style="text-align: left;"><img style="float: right; padding: 10px;"  src="https://radcliffe-harvard-edu.imgix.net/a7bba5e2-7668-44eb-be5d-03bed343dc0b/Timnit-Gebru_COURTESY.jpg?auto=compress%2Cformat&fit=min&fm=jpg&q=80&rect=421%2C0%2C1529%2C1528" alt="CelSci Image" width="292" height="292"/></p>
    <div style="text-align: left;">
  
     <p>
      In the field of AI , Dr. Timnit Gebru is a personality that stands out. She is a Computer Scientist and Political activist who earned her a Bachelor and Master of Science degree in electrical engineering and a PhD in computer vision at Stanford University. 
      Timnit Gebru has lent her skills to big tech companies such as Apple, Microsoft and Google.<sup> <a href="#ref1">1</a></sup>
    </p>
    <p>
      In this project, we will look at some of the insights Dr. Gebru shares in concern to the field of AI. 
      I will be discussing some research papers she has authored as well as five videos of Timnit Gebru. Gebru does not seem to have her own YouTube channel so the videos I will be referncing are talks or lectures that she has given as well as interviews.
    </p>
    <p>
      In these interviews there is a common theme, she typically talks of the dangers of encoding our biases in AI models and shares her insight on avanues to mitigate the harm these biases can cause.
      In the current state of technological advances with AI and Large Language Models, Timnit Gebru is seen as a controversial CelSci (Celebrity Scientist) especially because she does not hesitate to openly talk about her perspective not only in talks and academic spaces but especially on social media sites,
       she adeptly bridges the gap between the advancing tech world, who's leaders portray themselves to be stoic and the more morally conscious general population
    </p>

    <p>
      Through listening to her point of view, in this project I hope you will be able to learn more about the current problems at hand with AI, what we can do as students of Digital Humanites, how we can protect ourselves from the dangers
      as end users and the responsibilites of companies and researchers who have a hand in developing said technolgy.
    </p>
    </div>
    
<h1 id="Videos">Videos & Discussion</h1>
<button class="accordion"><h2 id="Utopia">1. Eugenics and the Promise of Utopia through AGI</h2></button>
<div class="panel">
<p><iframe width="700" height="355" style="float: left; padding: 10px;" src="https://www.youtube.com/embed/P7XT4TWLzJw?si=1LDRViWg-Z_zU1tw" title="Utopia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div style="text-align: left;">
  <p>In this video, Timnit Gebru describes the current tech climate and the rise of AGI ( Artificial General Intelligence) likening it to a second wave of eugenics. She discuss the historical roots of the first wave movement who&rsquo;s aim was &ldquo;improving the human stock&rdquo; and highlighted all the injustices people who supported this movement perpetrated and warns us of the dangers of making the same mistake a second time.</p>
  <p>Timnit highlights this second wave of eugenists (from 1990s to present) who, instead of making policies and taking blatantly violent recourses like those in the first wave, use biotechnology to &ldquo;design&rdquo; children but it&rsquo;s not viewed as the problem it is because they are not coercing the population(like in the first wave) and using technology.</p>
  <p>Émile Torres is a researcher Timnit collaborates with often. Émile coined the term TESCREAL and in this video she defines what the term means and talk about how all of these associated views can be problematic.</p>
  <p><strong>Transhumainsm :</strong> Julian Huxley was president of the British eugenics society and says that the point is for us to transcend humanity, modern transhumanism is like post humanism but to make a superior human.</p>
  <p><strong>Extropians : </strong> are the worlds first organised transhumanist movement.</p>
  <p><strong>Singualritanism : </strong>  is people who believe that at one point humans and machines will merge and that we won&rsquo;t be able to understand machines anymore because technology will be evolving quickly. Intelligence explosion whee algorithms undergo self improvement until they become super intelligent.</p>
  <p><strong> Cosmist : </strong> manifesto says that humans will merge technology and spread to the stars, they say that transhumans</p>
  <p><strong> Rationalism </strong></p>
  <p>Extropianism, singularitarianism, and cosmism are just variations of transhumanism.</p>
  <p><strong> Effective Altruism : </strong> similar to rationalism, doing the most good possible with finite resources. The initial goal was with current problems such as global poverty but now they are thinking lots of years into the future.</p>
  <p><strong> Longtermism : </strong> Longtermism is the ethical view that positively influencing the long-term future is a key moral priority of our time.</p>
  <p>Timnit gives some examples of TESCREALs to be persons like Bostrom, Yudkowsky, MacAskill, Musk, Altman and Bankman-Fried</p>
  <p>She says that the historical roots of these categories overlap and all lead to first wave eugenicists and that they all want to radically change the human form. 
    They are very interested in eschatology (the ultimate end of the universe), understanding their logic with utopian ideals and thinking that this “end” will create a perfect world but they
     also think that this same technology might bring an apocalypse.
    They believe that it is our priority to both build utopia and avoid apocalypse, ultimately focusing on the end</p>
  <p>They have highlighted their discriminatory views using outdated metrics such as IQ tests. They introduced a novel metric, PELTIV (Potential Expected Long-Term Instrumental Value) as a metric to assess individual
    members of the community to know "who will be more likely to develop high 'dedication' to EA” (Effective Altruism)</p>
  <p>This metric is very clearly discrimnatory because a Low PELTIV value was assigned to applicants who worked to reduce global poverty or mitigate climate change, while the highest value was assigned to those who directly worked for EA organizations or on artificial intelligence.
    </p>

  </p>
</div>
</div>

<br>

<button class="accordion"><h2 id="Dark">2. Whistleblowers Warn About the Dark Side of AI</h2></button>
<div class="panel">
<p><iframe width="700" height="355" style="float: right; padding: 10px;" src="https://www.youtube.com/embed/QHlacIwhZVo?si=hOH9r7UnGfusQ5Zy" title="Utopia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div style="text-align: left;">
  <p>In this video, Emily Chang Interviews Ifeoma Ozoma, former Pinterest policy manager, she said a colleague made her aware of a pay disparity and she brought it up and as a result she lost her job. She also interviews Timnit Gebru and Safiya Noble.</p> 
  <p>Timnit Gebru has been blowing the whistle about biases in ai research, while she worked at Google she authored a paper talking about the biases in ChatGpt and BARD, she says she got fired from google but google managers say she resigned.</p>
  <p>They mention that it is common that others think that they are being luddites and opposing technological advances, but it's the complete opposite. They are not anti Technology but pro human dignity.</p>
  <p>They keep the public from truly understanding what happens and that it’s just a black box to the users. Algorithms are dangerous when users don’t have a choice to opt in or out, they talk about entities such as insurance companies and mortgage companies that use algorithms to determine what benefits they allocate to a person and the dangers that come with that, these dangers are very easy to predict.</p>
  <p>Timnit dislikes that OpenAI is becoming a central power in AI. Where their GPT models are being built on for everything, where if you need a doctor or a lawyer you can talk to <strong> their</strong>  chatbot.They say that we can begin by disentangling healthcare from employment so the employees can freely and openly talk about the problems. We need to work against the concerted efforts of companies to make technology mysterious and magical to the users so they are oblivious to the fact that their data is being stolen. 
    The interviewees all agree that  doom mongering or doomers are not entirely wrong. They don’t mean in the sense of sci-fi literature but rather the fact that people are having high sentences from courts and no access to healthcare (because of insurance algorithms), more realistic matters.</p>
  <p> </p>
</div>
</div>

<br>

<button class="accordion"><h2 id="Google">3. AI Ethics Researcher Timnit Gebru's Firing Doesn’t Look Good For Google</h2></button>
<div class="panel">
<p><iframe width="700" height="355" style="float: left; padding: 10px;" src="https://www.youtube.com/embed/fWqpPsMXVsI?si=YlrHfqsaOK_6DKgG" title="Utopia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div style="text-align: left;">
  <p>
    In this interview they discus the fact that Timnit Gebru no longer works at Google. Google maintains that Timnit Gebru resigned but she insists that she was fired. 
All of this happened during the publishing of her paper “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜” as it was critiquing AI from OpenAI, Google and some other companies, she said that she had proposed to take her name off the paper and retract the paper.
Some of her colleagues at google resigned as well and made petitions to protest her firing.
  </p>  
  <p>
    She insists that her and her co authors weren’t talking about google specific technology and the cited papers from many other big tech companies. 
    In the paper they talk about how they don’t think that it will make much of a difference to create very big Language Models and about the environmental risks and that we should focus on the current issues with yes we have. 
     </p>
  <p>
    Timnit mentions that she does not appreciate the fact that people who work on this kind of novel technology always seem to forget minority groups, as a person who develops this technology, you need to consider many point of views to be able to identify the risks. 
    She was part of the Google Ethical AI team, but she does not think they were listened to much. She says that the team was seen as firefighters or some sort of damage control when they could have spent the time to do some actual research and to brainstorm solutions. 
    She mentions that when papers gained more traction externally was usually when Google agreed to make changes. But as she had made the team, she said they made a paper on gender recognition ai technology that did have an impact and that they tried.
  </p>

</div>
</div>

<br>

<button class="accordion"><h2 id="Limitations">4. Understanding the Limitations of AI: When Algorithms Fail</h2></button>
<div class="panel">
<p><iframe width="700" height="355" style="float: right; padding: 10px;" src="https://www.youtube.com/embed/aKf6pB4p06E?si=RAthZaiFwhFfYfAn" title="Utopia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div style="text-align: left;">
  <p>In this video timing Gebru talks about actual crime rates versus predicted cremates by algorithms. 
    For example prediction about drug related crimes. This type of data uses reported drug use and not <strong>actual</strong> use of drugs. 
    She says that crimes are typically more reported in certain areas that a are disadvantaged and that it makes sense to use that but it’s not entirely the truth.
  </p>
  <p>
    She signed a letter against “Extreme Vetting Initiative” where immigration systems enforcement was proposing to partner with big tech companies to scan social media profiles to analyse peoples social media activity to determine if someone will be a good immigrant or to see who will be a good citizen, Timnit is a refugee and she said this is something she holds close to heart.
  </p>
  <p>
    She talks about translation problems with translation tools and the biases that are encoded into these systems. 
    They result in people being imprisoned due to this kind of bias. She talks about problems with face recognition software saying that we can’t ignore social and structural problems such as racism being encoded into these things to have further insight the technology will be used and how it will be benefitting and not.
  </p>
  <p>
    Again brings up that there are no laws and restrictions, equal employment laws and law enforcement breaking laws. She says datasheets should be provided for this technology just as it is provided for hardware parts, about the ideal uses and things like that. 
    She talks about drugs and women not being allowed to partake in clinical trials of drugs and she thinks that we should learn from these mistakes in the past. These drugs were pulled from the market because they were affecting women negatively.
  </p>
</div>
</div>

<br>

<button class="accordion"><h2 id="EthicalAI">5. The Quest for Ethical Artificial Intelligence</h2></button>
<div class="panel">
<p><iframe width="700" height="355" style="float: left; padding: 10px;" src="https://www.youtube.com/embed/b_--xrN3eso?si=b1DN6eDqucu7qGHs" title="Utopia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div style="text-align: left;">
  <p>
    She encourages people to unionise and says that it’s unfair that students and researchers careers are controlled by how liked they are and says that if they are unionised they are able to refuse to work on unethical projects.
  </p>  
  <p>
    Timnit says that it’s unfortunate that most of the AI being developed is being funded by military groups to be used by them. It’s unfair that they are able to use a large amount of money to fuel their interest of hurting people more efficiently. 
    Corporations are more interested in making a profit than actually imparting goodness. Researchers should strive for community and not exploitative research, parachute researching is what she calls it. She says that research is generally a very exploitative endeavour and that she wants those groups to be supported and actually benefit from the research and just not be consulted when informations is needed.
  </p>
  <p>
    She says that a lot of ethics things related to AI are tied to labour laws because she thinks that many corporations go into AI to automate systems to avoid paying people, though it’s not at that stage yet where everything is automated, she brings up Kenyan workers being exploited by OpenAI, being paid at a meagre rate of $2 an hour and says that silicon valley is creating a global underclass because of this.
  </p>
  <p>
  There should be more regulations and better labour laws so everything is fair and ethical from this point of view.
  </p>
</div>
</div>


<h1 id="Research">Research</h1>

<h4 id="Responsible" style="text-decoration-line: underline; text-decoration-style: dashed;">A Human Rights-Based Approach to Responsible AI.</h4>
<h5>Prabhakaran, V., Mitchell, M., Gebru, T. and Gabriel, I. (2022). </h5>
<p>
  Fairness, accountability, transparency and ethics (FATE) research in AI is what Timnit and her co-authors suggest as a solution to biases they also bring up questions around how AI models might be problematically biased, unfair, or unethical, and how to make them “fairer” and more “ethical”.
</p>
<p>
  The research community working in this area greatly lacks in terms of geo-cultural diversity so most of the research produced is primarily framed in a Western lens.
  The authors of this paper push for regulation around AI, they give the example of UDHR (Universal Declaration of Human Rights) by the UN and say that it is an extensively detailed list of what every human being is entitled to, they give examples such as the right to life, liberty and security, right to privacy, right to be free of discrimination, and right to freedom of expression and that these qualities are important to build ethical AI systems.
</p>
<p>
  They mention that there is. Current research going on to identify ways to make AI research support human rights practices. The paper says that the efforts include trying to improve “human rights reporting in conflict zones using automated analysis of satellite imagery”.
</p>
<p>
  Authors maintain that science should be geared toward fulfilling human rights and advancements should benefit the greater population and should not be kept only for those in the west or of a certain economic class. They say that science should be shared, science has no borders, it’s every scientist responsibility to contribute to shared knowledge.
</p>
<p>

Human Rights can be an avenue for AI research, general society and policy makers to have a common language to communicate effectively about matters concerning AI. It can be a way to close the knowledge gap so all of these sides can gain insight from one another.
</p>


<h4 id="Artists" style="text-decoration-line: underline; text-decoration-style: dashed;">AI Art and its Impact on Artists.</h4>
<h5>Jiang, H., Brown, L.T., Cheng, J., Khan, M., Gupta, A., Workman, D., Hanna, A., Flowers, J. and Gebru, T. (2023)</h5>
<p>
  The topic of AI “Art” is controversial. The authors discuss what this kind of technology means for Artists and also discuss what we count as art. AI Art models like DALLE take in text as input and output images. They say that are is a uniquely human activity, we are anthropomorphising AI by calling some of these models artists and also by describing them as being “inspired” by their training data (which consists of stolen artwork) both of these things are misguided and harmful.
</p>
<p>
  The authors mention that the philosopher John Dewey says that an art style is developed throng interactions with a cultural environment and not sampling being mimicry from the training data an AI model is exposed to.
</p>
<p>
  They give the example of Claude Monet Nymphéas [Water Lilies] <img style="float: right; margin-right: 10px;" src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/%27White_Water_Lilies%27_by_Claude_Monet%2C_1899%2C_Pushkin_Museum.JPG/400px-%27White_Water_Lilies%27_by_Claude_Monet%2C_1899%2C_Pushkin_Museum.JPG" alt="White Water Lilies by Claude Monet"> 
  series of painting that he made after he lost his son. His experience of this loss, as a human inspired this series of artwork so it has a sort of sentiment and meaning. The same is true for other artists, their art is unique to the experiences, influences and environment.
</p>
<p>
  AI image generators cause harm to artists because some corporations or individuals see the output of these generators as “good enough” and just don’t work with artists as much anymore. They mention as well that the CEO of Sability AI accused artists on wanting to have a “monopoly on visual communications” and “skill segregation”.
</p>
<p>
  OpenAI has no disclosed the data set that was used for DALLE and that raises doubts about said data because we don't know if it was trained on only free use images or if the training data included copyrighted images as well. They also mention data laundering and that it might be something that AI companies partake in.Some artists feel that  their copyrighted images have been made into training data for these things and some have managed to even prove this. All of these terrible practices with data and the work of others causes artists to be hesitant to share their work and also to post tutorials for other artists to learn, the authors say that this limits the creativity of humans as a whole.
</p>
<p>
  It’s hard to purse cases of theft of art in this kind of scenario in a legal setting because there aren’t many laws about this.They still believe that AI generators can still be a medium to express oneself as an artist, using the example of Anna Ridler, who created the piece “Mosaic Virus” who trained an AI Image generator with 10,000 phots of tulips she had taken herself.<iframe style="padding: 10px; border: none;" src="https://emare.eu/works/mosaic-virus-2018" width="100%" height="400"></iframe>
</p>
<p>
  
</p>
<h1 id="SocMed">Social Media</h1>
<p>Timnit Gebru is a CelSci who’s very active on social media, namely on Twitter/X here are a few of her tweets and an analysis of misinformation using a website we saw in class.</p>
<img src="images/Tweet1.png" alt="Image 1" style="width:25%; margin: 0px;">
<img src="images/Tweet2.png" alt="Image 2" style="width:25%; margin: 0px;">
<img src="images/Tweet3.png" alt="Image 3" style="width:25%; margin: 0px;">
<img src="images/Tweet4.png" alt="Image 4" style="width:25%; margin: 0px;">
<img src="images/Tweet5.png" alt="Image 5" style="width:25%; margin: 0px;">
<p>Here is the result of using a website that shows how much information a Twitter account is exposed to, it determines this by checking the public figures the account follows and how much 
  fact checked information those public figures share
</p>
<img src="images/TwitAPI.png" alt="Image 1" style="width:100%; margin: 0px;">
<p>This website says that she is exposed to a lot less misinformation than other users</p>
<h1 id="Analysis">Analysis</h1>
<p>These is an analysis of the views each of the videos I selected for this project , video number 3 <strong>Ethics Researcher Timnit Gebru's Firing Doesn’t Look Good For Google</strong> has the most views whereas video number 5 <strong>The Quest for Ethical Artificial Intelligence</strong> has the least views, this may be because it's almost 
  one hour long.<p>

<div id="chartContainer" style="height: 600px; width: 80%; text-align: center; margin: 0 auto;"></div>
 <p> Here's another way to visualise this data, in a table instead of a chart, the cells highlighted in blue are the highest values in each column</p>
<gradio-lite>
  import gradio as gr
  import pandas as pd
  def show_df(as1,as2,as3):
     df = pd.DataFrame({
        "Videos" : ["How To Stop Artificial Intelligence From Marginalizing Communities", "Whistleblowers Warn About the Dark Side of AI ", "AI Ethics Researcher Timnit Gebru's Firing Doesn’t Look Good For Google", "Understanding the Limitations of AI: When Algorithms Fail", "The Quest for Ethical Artificial Intelligence"], 
        "Views" : [80827, 29428 , 107852, 9755, 7337], 
        "Duration (mins)" : [13.67, 24.03, 12.25, 15.567, 58.71]})
     print(as1)
     df = df.style.highlight_max(color = 'lightblue', axis = 0)
     return df
  with gr.Blocks() as demo:
    with gr.Row():
      with gr.Column():
        data = gr.Dataframe()
        demo.load(show_df, None, [data])
  demo.launch()
</gradio-lite>

<details>
<summary style= "font-family: Georgia, serif; font-size: 24px; font-weight: bold;">Here's a Chat Bot!</summary>
<gradio-lite>
  import gradio as gr
  import random
  import time

  with gr.Blocks() as demo:
      chatbot = gr.Chatbot()
      msg = gr.Textbox()
      clear = gr.Button("Clear")
      def vote(data: gr.LikeData):
         if data.liked:
             print("You upvoted this response: " + data.value)
         else:
             print("You downvoted this response: " + data.value)
      def user(user_message, history):
          return "", history + [[user_message, None]]

      def bot(history):
          bot_message = random.choice(["Dr. Timnit Gebru is a Computer Scientist and Political activist.", "LLMs do not truly understand what they are outputting", "early Automobile testing, when seatbelts would only be tested on men", "There should be more laws to regulate AI", "This is my final ACTION!"])
          history[-1][1] = ""
          for character in bot_message:
              history[-1][1] += character
              time.sleep(0.05)
              yield history

      msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(
          bot, chatbot, chatbot
      )
      clear.click(lambda: None, None, chatbot, queue=False)
      chatbot.like(vote, None, None)  # Adding this line causes the like/dislike icons to appear in your chatbot


  demo.queue()
  demo.launch()
</gradio-lite>
</details>

<h1 id="Reflection">Reflection</h1>
<p>I find Timnit’s research very interesting and I think that the topics she brings up are ones that we should have in mind when we’re interacting with or developing AI technology.  Because Timnit is active on social media, she is consistently called a luddite and there’s a lot of harassment that is targeted towards her
   because she explicitly names other CelScis who she thinks are contributing to bad ethical practices (for example, Elon Musk). </p>
<p>Open AI, Facebook and other companies out sourcing moderation for the products while exploiting their workers (by underpaying and overworking) in other 
  countries is a terrible practice. It’s especially heinous because it’s specifically done they know that these employees have almost no protections under the 
  (often times non existent)labour laws in their countries. Moderating online platforms to ensure safety is a very tasking job, moderators are exposed to a lot of sensitive and inappropriate content, some moderators say that it’s very a 
  traumatic experience for them to consistently be exposed to violent images (Niza Nondo ,2023).
</p>
<p>I agree with the idea that there should be data sheets for recommended uses of AI technology and explicit rules of what’s not allowed. 
  It can help stop the production of sensitive images of others without their consent and also reduce the theft of other’s work. 
  Companies should as well be transparent about what’s included in their training data. Mentioning that AI Image generators only generate images based on imitation and that this doesn’t 
  full encompass the human experience of learning through Adaptation and Conditioning as well (like we saw in class) brings forth more thoughts about what we can truly define art to be.
</p>

<h1 id="References">References</h1>
<p><i>I sourced some of the code used in this project(specifically the side bar and the drop down menus) from W3schools.com and used chatGPT for the syntax of the attribute values,an example of a prompt used is</i> "&lt;p&gt;&lt;frame style="text-align: center; padding: 10px; border: none;"&gt;..... why does the alignment not work?"</p>
    <div style="text-align: left;">
      <ol>
        <li>Bloomberg Originals (2023). Whistleblowers Warn About the Dark Side of AI | The Circuit with Emily Chang. 
          [online] www.youtube.com. Available at:  <a href= "https://youtu.be/QHlacIwhZVo?si=4OKyXNEc4RQc1acE">https://youtu.be/QHlacIwhZVo?si=4OKyXNEc4RQc1acE</a>.</li>
        <br>
        <li>Databricks (2019). Understanding the Limitations of AI: When Algorithms Fail | Timnit Gebru (Google Brain). 
          [online] www.youtube.com. Available at: <a href= "https://www.youtube.com/watch?v=aKf6pB4p06E&t=1s">https://www.youtube.com/watch?v=aKf6pB4p06E&t=1s</a>.</li>
        <br>
        <li>Harvard Radcliffe Institute (2022). The Quest for Ethical Artificial Intelligence | Timnit Gebru || Harvard Radcliffe Institute. 
          [online] www.youtube.com. Available at:  <a href= "https://www.youtube.com/watch?v=b_--xrN3eso">https://www.youtube.com/watch?v=b_--xrN3eso</a>.</li>
        <br>
        <li>Jiang, H., Brown, L.T., Cheng, J., Khan, M., Gupta, A., Workman, D., Hanna, A., Flowers, J. and Gebru, T. (2023). 
          AI Art and its Impact on Artists. AI Art and its Impact on Artists. doi:<a href= "https://doi.org/10.1145/3600211.3604681">https://doi.org/10.1145/3600211.3604681</a>.</li>
        <br>
        <li>
          Niza Nondo (2023). Facing disturbing content daily, online moderators in Africa want better protections and a fair wage. [online] CBC. Available at:
          <a href="https://www.cbc.ca/radio/thecurrent/content-moderators-union-social-media-ai-1.6848949"> https://www.cbc.ca/radio/thecurrent/content-moderators-union-social-media-ai-1.6848949</a>.
        </li>
        <br>
        <li>Nicolas Papernot (2023). SaTML 2023 - Timnit Gebru - Eugenics and the Promise of Utopia through AGI. 
          [online] www.youtube.com. Available at: <a href= "https://www.youtube.com/watch?v=P7XT4TWLzJw">https://www.youtube.com/watch?v=P7XT4TWLzJw</a>.</li>
        <br>
        <li>Prabhakaran, V., Mitchell, M., Gebru, T. and Gabriel, I. (2022). 
          A Human Rights-Based Approach to Responsible AI. arXiv (Cornell University). doi:<a href= "https://doi.org/10.48550/arxiv.2210.02667">https://doi.org/10.48550/arxiv.2210.02667</a>.</li>
        <br>
        <li>VICE NEWS (2020). AI Ethics Researcher Timnit Gebru’s Firing Doesn’t Look Good For Google. 
          [online] www.youtube.com. Available at: <a href= "https://youtu.be/fWqpPsMXVsI?si=F7K3NMtblRlwDB82">https://youtu.be/fWqpPsMXVsI?si=F7K3NMtblRlwDB82</a>.</li>
        <br>
    </ol>
 
<script>
var dropdown = document.getElementsByClassName("dropdown-btn");
var i;

for (i = 0; i < dropdown.length; i++) {
  dropdown[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var dropdownContent = this.nextElementSibling;
    if (dropdownContent.style.display === "block") {
      dropdownContent.style.display = "none";
    } else {
      dropdownContent.style.display = "block";
    }
  });
}
var acc = document.getElementsByClassName("accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    } 
  });
}

</script>
<script type="text/javascript">
  window.onload = function () {
    var chart = new CanvasJS.Chart("chartContainer");
  
    chart.options.axisY = {suffix: "K" };
    chart.options.title = { text: "Timnit Gebru YouTube Stats" };
  
    var series1 = { //dataSeries - first quarter
        type: "column",
        name: "Views (in thousands)",
        color: "#93745E",
       
        showInLegend: true
    };
  
    var series2 = { //dataSeries - second quarter
        type: "column",
        name: "Duration in minutes",
        color: "#CCB69C",
      
        showInLegend: true
    };
  
    chart.options.data = [];
    chart.options.data.push(series1);
    chart.options.data.push(series2);
  
  
    series1.dataPoints = [
            { label: "How To Stop Artificial Intelligence From Marginalizing Communities", y: 80},
            { label: "Whistleblowers Warn About the Dark Side of AI ", y: 29},
            { label: "AI Ethics Researcher Timnit Gebru's Firing Doesn’t Look Good For Google", y: 107 },
            { label: "Understanding the Limitations of AI: When Algorithms Fail", y: 16 },
            { label: "The Quest for Ethical Artificial Intelligence", y: 9.7 }
    ];
  
    series2.dataPoints = [
        { label: "How To Stop Artificial Intelligence From Marginalizing Communities", y: 13.67 },
        {
          label: "Whistleblowers Warn About the Dark Side of AI", y: 24.03 },
        { label: "AI Ethics Researcher Timnit Gebru's Firing Doesn’t Look Good For Google", y: 12.25 },
        { label: "Understanding the Limitations of AI: When Algorithms Fail", y: 15.567 },
        { label: "The Quest for Ethical Artificial Intelligence", y: 58.71 }
    ];
      
      chart.options.axisX = {
          interval: 1,
          labelFontSize: 12,
      };
  
    chart.render();
  }
  </script>
<script type="text/javascript" src="https://cdn.canvasjs.com/canvasjs.min.js"></script>
<script type="module" crossorigin src="https://cdn.jsdelivr.net/npm/@gradio/lite/dist/lite.js"></script>

<div style="background-color: #362B21; color: white; text-align: right;padding: 10px;">
  <p>Leila G. D.<br>
    101196770<br>
    CGSC 3704<br>
    Prof. Ahmad Sohrabi<br>
    Dec 2023
  </p>
  </div>

</body>
</html> 
